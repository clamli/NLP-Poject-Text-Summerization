{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36_64\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import *\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from rouge import Rouge\n",
    "from data import *\n",
    "from utils import *\n",
    "from topic_model import *\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "e.g.\n",
    "```\n",
    "[[\"Musicians to tackle US red tape Musicians ' groups are to tackle US visa regulations which are blamed for hindering\",\n",
    "  \"Nigel McCune from the Musicians ' Union said British musicians\"],\n",
    " [\"U2 's desire to be number one U2 , who have won three prestigious Grammy Awards for their hit Vertigo\",\n",
    "  'But they still want more.They have to want to be'],\n",
    " [\"Rocker Doherty in on-stage fight Rock singer Pete Doherty has been involved in a fight with his band 's guitarist\",\n",
    "  'Babyshambles , which he formed after his acrimonious departure from']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "data = load_datasets('./Datasets/BBC_News_120_60.pkl')\n",
    "train = data[0:(int)(0.8*len(data))]\n",
    "dev = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in train:\n",
    "#     pair[0] = pair[0].split(' ')[0]\n",
    "#     pair[1] = pair[1].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[0:10]\n",
    "# dev = dev[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens Index\n",
    "e.g.\n",
    "```\n",
    "[Musicians to tackle US red tape Musicians ' groups are to tackle US visa regulations which are blamed for hindering => Nigel McCune from the Musicians ' Union said British musicians\n",
    "    indexed as: [2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 3, 4, 5, 11, 12, 13, 10, 14, 15, 16] => [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2],\n",
    " U2 's desire to be number one U2 , who have won three prestigious Grammy Awards for their hit Vertigo => But they still want more.They have to want to be\n",
    "    indexed as: [17, 18, 19, 3, 20, 21, 22, 17, 23, 24, 25, 26, 27, 28, 29, 30, 15, 31, 32, 33] => [13, 14, 15, 16, 17, 18, 19, 16, 19, 20, 2],\n",
    " Rocker Doherty in on-stage fight Rock singer Pete Doherty has been involved in a fight with his band 's guitarist => Babyshambles , which he formed after his acrimonious departure from\n",
    "    indexed as: [34, 35, 36, 37, 38, 39, 40, 41, 35, 42, 43, 44, 36, 45, 38, 46, 47, 48, 18, 49] => [21, 22, 23, 24, 25, 26, 27, 28, 29, 5, 2]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indexed, dev_data_indexed, vocab_indexer = index_datasets(train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data_indexed = train_data_indexed[500:]\n",
    "# train_data_indexed = train_data_indexed[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pickle.load(open('./topics_120_60_1/topic_word_30.pkl','rb'))    # topic_number, vocab_size\n",
    "topicword_topic = pickle.load(open('./topics_120_60_1/topicword_topic_30.pkl','rb'))     # topic_words_number, topic_number\n",
    "topicword2index = pickle.load(open('./topics_120_60_1/topicword2index_30.pkl','rb'))     # vocab set index : topic cwords index\n",
    "topic_vocab = pickle.load(open('./topics_120_60_1/topic_vocab_30.pkl','rb'))             # vocab set index\n",
    "# topic_word, topicword_topic, topicword2index, topic_vocab = getTopicEmbeddings(dataset, source, vo2index, num_topics=30)\n",
    "topic_word = torch.FloatTensor(topic_word)\n",
    "tokens = torch.LongTensor([i for i in range(len(vocab_indexer))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "- Pad the train/dev input vectors to the max length of the train/dev input documents.\n",
    "- Pad the train/dev output vectors to the max length of the train/dev output summerization.\n",
    "\n",
    "![](https://i.imgur.com/gGlkEEF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_input_tensor(exs, vocab_indexer, max_len):\n",
    "    return np.array([[ex.x_indexed[i] if i < len(ex.x_indexed) else vocab_indexer.index_of(PAD_SYMBOL)\n",
    "                        for i in range(0, max_len)] for ex in exs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_output_tensor(exs, vocab_indexer, max_len):\n",
    "    return np.array([[ex.y_indexed[i] if i < len(ex.y_indexed) else vocab_indexer.index_of(PAD_SYMBOL)\n",
    "                        for i in range(0, max_len)] for ex in exs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(input_array, batch_size=2, cuda=False):\n",
    "    input_batches = []\n",
    "    batch_num = (int)(input_array.shape[0] / batch_size)\n",
    "    start = 0\n",
    "    for i in range(batch_num):\n",
    "        batch = torch.from_numpy(input_array[start:start+batch_size, :])\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "        input_batches.append(batch)\n",
    "        start += batch_size\n",
    "    if start != input_array.shape[0]:\n",
    "        batch = torch.from_numpy(input_array[start:, :])\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "        input_batches.append(batch)\n",
    "    return input_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    # Parameters: dimension of the word embeddings, number of words, and the dropout rate to apply\n",
    "    # (0.2 is often a reasonable value)\n",
    "    def __init__(self, input_dim, full_dict_size, embedding_dropout_rate):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(embedding_dropout_rate)\n",
    "        self.word_embedding = nn.Embedding(full_dict_size, input_dim)\n",
    "\n",
    "    # Takes either a non-batched input [sent len x input_dim] or a batched input\n",
    "    # [batch size x sent len x input dim]\n",
    "    def forward(self, input):\n",
    "        embedded_words = self.word_embedding(input)\n",
    "        final_embeddings = self.dropout(embedded_words)\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-layer RNN encoder for batched inputs -- handles multiple sentences at once. You're free to call it with a\n",
    "# leading dimension of 1 (batch size 1) but it does expect this dimension.\n",
    "class RNNEncoder(nn.Module):\n",
    "    # Parameters: input size (should match embedding layer), hidden size for the LSTM, dropout rate for the RNN,\n",
    "    # and a boolean flag for whether or not we're using a bidirectional encoder\n",
    "    def __init__(self, input_size, hidden_size, dropout, bidirect, CUDA=False):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.CUDA = CUDA\n",
    "        self.bidirect = bidirect\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reduce_h_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
    "        self.reduce_c_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True,\n",
    "                               dropout=dropout, bidirectional=self.bidirect)\n",
    "        self.init_weight()\n",
    "\n",
    "    # Initializes weight matrices using Xavier initialization\n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_hh_l0, gain=1)\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_ih_l0, gain=1)\n",
    "        if self.bidirect:\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_hh_l0_reverse, gain=1)\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_ih_l0_reverse, gain=1)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0, 0)\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0, 0)\n",
    "        if self.bidirect:\n",
    "            nn.init.constant_(self.rnn.bias_hh_l0_reverse, 0)\n",
    "            nn.init.constant_(self.rnn.bias_ih_l0_reverse, 0)\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return self.hidden_size * 2 if self.bidirect else self.hidden_size\n",
    "\n",
    "    def sent_lens_to_mask(self, lens, max_length):\n",
    "        return torch.from_numpy(np.asarray([[1 if j < lens.data[i].item() else 0 for j in range(0, max_length)] for i in range(0, lens.shape[0])]))\n",
    "\n",
    "    # embedded_words should be a [batch size x sent len x input dim] tensor\n",
    "    # input_lens is a tensor containing the length of each input sentence\n",
    "    # Returns output (each word's representation), context_mask (a mask of 0s and 1s\n",
    "    # reflecting where the model's output should be considered), and h_t, a *tuple* containing\n",
    "    # the final states h and c from the encoder for each sentence.\n",
    "    def forward(self, embedded_words, input_lens):\n",
    "        # Takes the embedded sentences, \"packs\" them into an efficient Pytorch-internal representation\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_words, input_lens, batch_first=True)\n",
    "\n",
    "        # Runs the RNN over each sequence. Returns output at each position as well as the last vectors of the RNN\n",
    "        # state for each sentence (first/last vectors for bidirectional)\n",
    "        output, hn = self.rnn(packed_embedding)\n",
    "#         print(len(hn))\n",
    "#         print('hn[0]: ', hn[0].shape)\n",
    "#         print('hn[1]: ', hn[1].shape)\n",
    "        \n",
    "\n",
    "        # Unpacks the Pytorch representation into normal tensors\n",
    "        output, sent_lens = nn.utils.rnn.pad_packed_sequence(output)\n",
    "#         print('kdjfksdjfs: ', output.shape)\n",
    "        \n",
    "        # print('input_lens:', input_lens)\n",
    "        max_length = input_lens.data[0].item()\n",
    "        context_mask = self.sent_lens_to_mask(sent_lens, max_length)\n",
    "        if self.CUDA:\n",
    "            context_mask = context_mask.cuda()\n",
    "\n",
    "        # Grabs the encoded representations out of hn, which is a weird tuple thing.\n",
    "        # Note: if you want multiple LSTM layers, you'll need to change this to consult the penultimate layer\n",
    "        # or gather representations from all layers.\n",
    "        if self.bidirect:\n",
    "            h, c = hn[0], hn[1]          # [2, 20, 200]\n",
    "            # print('encoder hidden:----- ', h.shape)\n",
    "            # print('encoder cell:----- ', c.shape)\n",
    "            # Grab the representations from forward and backward LSTMs\n",
    "            h_, c_ = torch.cat((h[0], h[1]), dim=1), torch.cat((c[0], c[1]), dim=1)      # [20, 400]\n",
    "            # print('kdjfksdddddddddddjfs: ',h_.shape)\n",
    "            # Reduce them by multiplying by a weight matrix so that the hidden size sent to the decoder is the same\n",
    "            # as the hidden size in the encoder\n",
    "            new_h = self.reduce_h_W(h_)\n",
    "            new_c = self.reduce_c_W(c_)\n",
    "            h_t = (new_h, new_c)\n",
    "        else:\n",
    "            h, c = hn[0][0], hn[1][0]\n",
    "            h_t = (h, c)\n",
    "            \n",
    "#         print(len(hn))\n",
    "#         print('ht[0]: ', h_t[0].shape)\n",
    "#         print('ht[1]: ', h_t[1].shape)\n",
    "#         print('output: ', output.shape)\n",
    "        return (output, context_mask, h_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-based Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnRNNDecoderBahdanau(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout, bidirect):\n",
    "        super(AttnRNNDecoderBahdanau, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "#         self.sent_lens = sent_lens\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.bidirect = bidirect\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, \n",
    "                                dropout=dropout, bidirectional=bidirect)\n",
    "        \n",
    "        \n",
    "        self.context = nn.Linear(hidden_size * 2 + input_size, input_size)\n",
    "        self.W_h = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.W_c = nn.Linear(1, hidden_size * 2, bias=False)\n",
    "        self.W_s = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.v = nn.Linear(hidden_size * 2, 1, bias=False)\n",
    "        self.V = nn.Linear(hidden_size * 2 + hidden_size, hidden_size)\n",
    "        self.V_p = nn.Linear(hidden_size, output_size)\n",
    "        self.P_gen_layer = nn.Linear(hidden_size * 4 + input_size, 1)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    # Initializes weight matrices using Xavier initialization\n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_hh_l0, gain=1)\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_ih_l0, gain=1)\n",
    "        if self.bidirect:\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_hh_l0_reverse, gain=1)\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_ih_l0_reverse, gain=1)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0, 0)\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0, 0)\n",
    "        if self.bidirect:\n",
    "            nn.init.constant_(self.rnn.bias_hh_l0_reverse, 0)\n",
    "            nn.init.constant_(self.rnn.bias_ih_l0_reverse, 0)\n",
    "\n",
    "    def forward(self, embedded_words, dec_hidden, enc_output, context_mask, pre_cont, coverage):\n",
    "#         print(coverage)\n",
    "        sent_lens = enc_output.shape[0]\n",
    "        enc_feature = enc_output.view(-1, 2*hidden_size)                # batch_size*sent_lens, 2*hidden_size\n",
    "        rnn_input = self.context(torch.cat((pre_cont, embedded_words), 1))   # batch_size, input_size\n",
    "        dec_output, hn = self.rnn(rnn_input.unsqueeze(1).transpose(0, 1), dec_hidden)   # 1, batch_size, hidden_size\n",
    "        dec_output = dec_output.transpose(0, 1)                             # batch_size, 1, hidden_size\n",
    "        h_dec, c_dec = hn\n",
    "        s_t_hat = torch.cat((h_dec.view(-1, hidden_size),\n",
    "                             c_dec.view(-1, hidden_size)), 1)           # batch_size, 2*hidden_size\n",
    "        del h_dec, c_dec\n",
    "        \n",
    "        # Attention Distribution\n",
    "        dec_state = self.W_s(s_t_hat)                              # batch_size, 2*hidden_size\n",
    "        dec_state_expanded = dec_state.unsqueeze(1).expand(dec_state.shape[0], sent_lens, dec_state.shape[1]).contiguous() # batch_size, sent_lens, 2*hidden_size\n",
    "        dec_state_expanded = dec_state_expanded.view(-1, dec_state.shape[1])  # batch_size*sent_lens, 2*hidden_size\n",
    "        del dec_state\n",
    "        \n",
    "#         print(enc_feature.shape)\n",
    "#         print(dec_state_expanded.shape)\n",
    "#         print(coverage.shape)\n",
    "        e = self.v(torch.tanh(self.W_h(enc_feature) + dec_state_expanded)).view(-1, sent_lens)  # batch_size, sent_lens\n",
    "#         e = self.v(torch.tanh(self.W_h(enc_feature) + dec_state_expanded + self.W_c(coverage.view(-1, 1)))).view(-1, sent_lens)  # batch_size, sent_lens\n",
    "#         att_feature = enc_feature + dec_state_expanded   # batch_size*sent_lens, 2*hidden_size\n",
    "#         # Coverage\n",
    "#         coverage_feature = self.W_c(coverage.view(-1, 1))  # batch_size*sent_lens, 2*hidden_size\n",
    "#         att_feature = att_feature + coverage_feature   # batch_size*sent_lens, 2*hidden_size\n",
    "#         e = torch.tanh(att_feature)       # batch_size*sent_lens, 2*hidden_size\n",
    "#         attn_scores = self.v(e).view(-1, self.sent_lens)      # batch_size, sent_lens\n",
    "        del enc_feature\n",
    "        attn_distrib_ = F.softmax(e, dim=1)*context_mask.float()   # batch_size, sent_lens\n",
    "        del e\n",
    "        norm_factor = attn_distrib_.sum(1, keepdim=True)\n",
    "        attn_distrib = attn_distrib_ / norm_factor\n",
    "        del attn_distrib_, norm_factor\n",
    "#         coverage = coverage + attn_distrib                  # batch_size, sent_lens\n",
    "        attn_distrib = attn_distrib.unsqueeze(1)            # batch_size, 1, sent_lens\n",
    "        \n",
    "        # Context Vector\n",
    "        cont_vec = torch.bmm(attn_distrib, enc_output.transpose(0, 1))  # batch_size, 1, 2*hidden_size\n",
    "        concat_input = torch.cat((dec_output, cont_vec), dim=-1)           # batch_size, 1, enc_hidden_size * num_directions + dec_hidden_size\n",
    "\n",
    "        # Vocabulary Distribution\n",
    "        vocab_distrib = torch.softmax(self.V_p(self.V(concat_input)), dim=-1).squeeze(1)  # batch_size, output_size\n",
    "\n",
    "        # Pointer Generator\n",
    "        P_gen_input = torch.cat((cont_vec.squeeze(1), s_t_hat, rnn_input), dim=1)  # batch_size, (2*2*hidden_size + input_size)\n",
    "        P_gen = self.P_gen_layer(P_gen_input)\n",
    "        P_gen = torch.sigmoid(P_gen)       # batch_size, 1\n",
    "        \n",
    "        return (P_gen, vocab_distrib, hn, attn_distrib.squeeze(1), cont_vec.squeeze(1), coverage)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnRNNDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout, bidirect):\n",
    "        super(AttnRNNDecoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.bidirect = bidirect\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, \n",
    "                                dropout=dropout, bidirectional=bidirect)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2 + hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "        self.W_h = nn.Linear(hidden_size * 2, 1)\n",
    "        self.W_s = nn.Linear(hidden_size, 1)\n",
    "        self.W_x = nn.Linear(input_size, 1)\n",
    "        self.init_weight()\n",
    "        \n",
    "    # Initializes weight matrices using Xavier initialization\n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_hh_l0, gain=1)\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_ih_l0, gain=1)\n",
    "        if self.bidirect:\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_hh_l0_reverse, gain=1)\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_ih_l0_reverse, gain=1)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0, 0)\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0, 0)\n",
    "        if self.bidirect:\n",
    "            nn.init.constant_(self.rnn.bias_hh_l0_reverse, 0)\n",
    "            nn.init.constant_(self.rnn.bias_ih_l0_reverse, 0)\n",
    "\n",
    "    # enc_output: batch_size, sent_lens, 2*hidden_size\n",
    "    def forward(self, embedded_words, dec_hidden, enc_outputs, context_mask):\n",
    "        embedded_words = embedded_words.view(1, embedded_words.size(0), embedded_words.size(1))   # 1, batch_size, input_size\n",
    "        context_mask = context_mask.type(torch.uint8).unsqueeze(1)      # batch_size, 1, sent_lens\n",
    "  \n",
    "        rnn_output, hn = self.rnn(embedded_words, dec_hidden)           # 1, batch_size, hidden_size\n",
    "        rnn_output = rnn_output.transpose(0, 1)                         # batch_size, 1, hidden_size        \n",
    "        \n",
    "        attn_scores = rnn_output.bmm(self.linear(enc_outputs).transpose(0, 1).transpose(1, 2))      # batch_size, 1, sent_lens\n",
    "        attn_scores.data.masked_fill(context_mask == 0, float('inf'))      # batch_size, 1, sent_lens\n",
    "#         print(attn_scores.shape)\n",
    "        \n",
    "        # Attention Distribution\n",
    "        attn_weights = F.softmax(attn_scores.squeeze(1), dim=1).unsqueeze(1)        # batch_size, 1, sent_lens\n",
    "        # Context Vector\n",
    "        context = attn_weights.bmm(enc_outputs.transpose(0, 1))                     # batch_size, 1, hidden_size * num_directions\n",
    "        concat_input = torch.cat((context, rnn_output), dim=-1)     # batch_size, 1, enc_hidden_size * num_directions + dec_hidden_size\n",
    "        concat_output = torch.tanh(self.concat(concat_input))                       # batch_size, 1, dec_hidden_size\n",
    "        # Vocabulary Distribution\n",
    "        output = self.out(concat_output).squeeze(1)                                 # batch_size, output_size\n",
    "        # Pointer-Generator\n",
    "        p_gen = torch.sigmoid(self.W_h(context) + self.W_s(rnn_output) + self.W_x(embedded_words.transpose(0, 1))).squeeze(1)  # batch_size, 1\n",
    "#         print(p_gen.shape)\n",
    "        \n",
    "        return (p_gen, output, hn, attn_weights.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder to Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_for_decoder(x_tensor, inp_lens_tensor, model_input_emb, model_enc, topic_embedding=None, pos_tensor=None):\n",
    "    input_emb = model_input_emb.forward(x_tensor)\n",
    "    \n",
    "    if TOPIC:\n",
    "        for pos in pos_tensor:\n",
    "            token_index = topicword2index[x_tensor[pos[0]][pos[1]]]\n",
    "            input_emb[pos[0]][pos[1]] = topicword_topic[token_index].mm(topic_embeddding)   # b*s*i\n",
    "#         for i in range(x_tensor.shape[0]):\n",
    "#             for j in range(x_tensor.shape[1]):\n",
    "#                 token = x_tensor[i][j]\n",
    "#                 if token in topic_vocab:\n",
    "#                     token_index = topicword2index[token]\n",
    "#                     input_emb[i][j] = topicword_topic[token_index].mm(topic_embeddding)\n",
    "    \n",
    "    (enc_output_each_word, enc_context_mask, enc_final_states) = model_enc.forward(input_emb, inp_lens_tensor)\n",
    "    enc_final_states_reshaped = (enc_final_states[0].unsqueeze(0), enc_final_states[1].unsqueeze(0))\n",
    "    return (enc_output_each_word, enc_context_mask, enc_final_states_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of loss function: masked cross entropy\n",
    "# Reference to https://github.com/spro/practical-pytorch, make some modifications\n",
    "def masked_cross_entropy(logits, target, length, context_mask):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))                  # batch_size * sent_len, vocab_size\n",
    "    log_probs_flat = F.log_softmax(logits_flat, dim=-1)             # batch_size * sent_len, vocab_size\n",
    "    target_flat = target.view(-1, 1)                                # batch * sent_len, 1\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)    # batch * max_len, 1\n",
    "    losses = losses_flat.view(*target.size())     # batch, max_len\n",
    "    losses = losses * context_mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_step_loss(final_distrib, attn_distrib, coverage, step_Y_tensor, step_context_mask):\n",
    "    step_distrib = torch.gather(final_distrib, dim=1, index=step_Y_tensor).squeeze(1)\n",
    "#     step_coverage_loss = torch.sum(torch.min(attn_distrib, coverage), 1)\n",
    "#     if ((step_distrib + eps) < 0).sum() > 0:\n",
    "#         sys.exit(0)\n",
    "#     step_loss = -torch.log(step_distrib + eps) + cov_loss_wt*step_coverage_loss\n",
    "    step_loss = -torch.log(step_distrib + eps)\n",
    "    step_loss = step_loss * step_context_mask\n",
    "    return step_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Copy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_generate_train(p_gen, dec_output, dec_attn, X_tensors, CUDA):\n",
    "#     final_distrib = torch.zeros(dec_output.shape, dtype=torch.float)     # batch_size, vocab_size\n",
    "    final_distrib = p_gen * dec_output\n",
    "    dec_attn_padding = torch.zeros(X_tensors.shape, dtype=torch.float)\n",
    "    if CUDA:\n",
    "        dec_attn_padding = dec_attn_padding.cuda()\n",
    "    dec_attn_padding[:, 0:dec_attn.shape[1]] = dec_attn \n",
    "    final_distrib = final_distrib.scatter_add(1, X_tensors, (1-p_gen)*dec_attn_padding)\n",
    "    return final_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "lr = 0.0005\n",
    "input_dim = 100\n",
    "output_dim = 100\n",
    "hidden_size = 256\n",
    "emb_dropout = 0.2\n",
    "rnn_dropout = 0.2\n",
    "bidirectional = True\n",
    "num_epochs = 290\n",
    "teacher_forcing_ratio = 1\n",
    "cov_loss_wt = 1.0\n",
    "eps = 1e-12\n",
    "CUDA = True\n",
    "TOPIC = False\n",
    "beam_size = 3\n",
    "N_GRAM = 1\n",
    "pad_idx = vocab_indexer.index_of(PAD_SYMBOL)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create indexed input/output for training**\n",
    "- X_tensors_batch/Y_tensors_batch, list[array: [batch_size, sent_len], batch_num]\n",
    "- inp_lens_batch/oup_lens_batch, list[array: [batch_size,], batch_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexed input/output for training\n",
    "train_data_indexed.sort(key=lambda ex: len(ex.x_indexed), reverse=True)\n",
    "input_train_max_len = np.max(np.asarray([len(ex.x_indexed) for ex in train_data_indexed]))\n",
    "all_train_input_data = make_padded_input_tensor(train_data_indexed, vocab_indexer, input_train_max_len).astype(np.int64)\n",
    "\n",
    "output_train_max_len = np.max(np.asarray([len(ex.y_indexed) for ex in train_data_indexed]))\n",
    "all_train_output_data = make_padded_output_tensor(train_data_indexed, vocab_indexer, output_train_max_len).astype(np.int64)\n",
    "\n",
    "X_tensors_batch = batch_data(all_train_input_data, BATCH_SIZE, cuda=CUDA)   # batch_num, batch_size, sent_len\n",
    "pos_tensor_batch = []\n",
    "for X_tensor in X_tensors_batch:\n",
    "    pos_tensor = []\n",
    "    for i in range(X_tensor.shape[0]):\n",
    "        for j in range(X_tensor.shape[1]):\n",
    "            token = X_tensor[i][j]\n",
    "            if token in topic_vocab:\n",
    "                pos_tensor.append([i, j])\n",
    "    pos_tensor_batch.append(pos_tensor)\n",
    "Y_tensors_batch = batch_data(all_train_output_data, BATCH_SIZE, cuda=CUDA)  # batch_num, batch_size, sent_len\n",
    "\n",
    "if CUDA:\n",
    "    inp_lens_batch = [torch.tensor([torch.sum(X_tensor != 0) for X_tensor in X_tensors]).cuda() for X_tensors in X_tensors_batch]  # batch_num, batch_size\n",
    "    oup_lens_batch = [torch.tensor([torch.sum(Y_tensor != 0) for Y_tensor in Y_tensors]).cuda() for Y_tensors in Y_tensors_batch]  # batch_num, batch_size\n",
    "else:\n",
    "    inp_lens_batch = [torch.tensor([torch.sum(X_tensor != 0) for X_tensor in X_tensors]) for X_tensors in X_tensors_batch]  # batch_num, batch_size\n",
    "    oup_lens_batch = [torch.tensor([torch.sum(Y_tensor != 0) for Y_tensor in Y_tensors]) for Y_tensors in Y_tensors_batch]  # batch_num, batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**\n",
    "- model_input_emb/model_output_emb: embedding layer\n",
    "- model_enc/model_dec: encoder/decoder\n",
    "- optimizers: encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36_64\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model_input_emb = EmbeddingLayer(input_dim, len(vocab_indexer), emb_dropout)\n",
    "model_enc = RNNEncoder(input_dim, hidden_size, rnn_dropout, bidirectional, CUDA=CUDA)\n",
    "model_output_emb = EmbeddingLayer(output_dim, len(vocab_indexer), emb_dropout)\n",
    "model_dec = AttnRNNDecoderBahdanau(input_size=output_dim, hidden_size=hidden_size, output_size=len(vocab_indexer), dropout=rnn_dropout, bidirect=False)\n",
    "# CUDA\n",
    "if CUDA:\n",
    "    model_input_emb.cuda()\n",
    "    model_enc.cuda()\n",
    "    model_output_emb.cuda()\n",
    "    model_dec.cuda()\n",
    "# model_dec = AttnRNNDecoder(input_size=output_dim, hidden_size=hidden_size, output_size=len(vocab_indexer), dropout=rnn_dropout, bidirect=False)\n",
    "enc_optimizer = optim.Adam(model_enc.parameters(), lr=lr)\n",
    "dec_optimizer = optim.Adam(model_dec.parameters(), lr=lr)\n",
    "if CUDA:\n",
    "    tokens = tokens.cuda()\n",
    "    topic_word = topic_word.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "losses = []\n",
    "for epoch in range(0, num_epochs):\n",
    "    print('--------------------- Epoch %d ---------------------'%(epoch+1))\n",
    "    for X_tensors, pos_tensor, Y_tensors, inp_lens_tensor, oup_lens_tensor in zip(X_tensors_batch, pos_tensor_batch, Y_tensors_batch, inp_lens_batch, oup_lens_batch):\n",
    "\n",
    "        model_enc.train()\n",
    "        model_dec.train()\n",
    "\n",
    "        enc_optimizer.zero_grad()\n",
    "        dec_optimizer.zero_grad()\n",
    "        \n",
    "        # # Encoder\n",
    "        if TOPIC:\n",
    "            tokens_embedding = model_input_emb.forward(tokens)\n",
    "            topic_embedding = topic_word.mm(tokens_embedding)\n",
    "            enc_outputs, enc_context_mask, enc_hidden = encode_input_for_decoder(X_tensors, inp_lens_tensor, model_input_emb, model_enc, topic_embedding, pos_tensor)\n",
    "        else:\n",
    "            enc_outputs, enc_context_mask, enc_hidden = encode_input_for_decoder(X_tensors, inp_lens_tensor, model_input_emb, model_enc)\n",
    "        init_dec_inp = Variable(torch.LongTensor([vocab_indexer.index_of(SOS_SYMBOL)] * X_tensors.shape[0]))  \n",
    "        if CUDA:\n",
    "            init_dec_inp = init_dec_inp.cuda()\n",
    "        dec_input = model_output_emb.forward(init_dec_inp)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        print(dec_hidden[0].shape)\n",
    "        cont = torch.zeros((X_tensors.shape[0], 2 * hidden_size))    # batch_size, 2*hidden_size\n",
    "        coverage = torch.zeros((X_tensors.shape[0], enc_outputs.shape[0]))             # batch_size, sent_lens\n",
    "        all_context_mask = torch.from_numpy(np.asarray([[1 if j < oup_lens_tensor.data[i].item() \\\n",
    "            else 0 for j in range(0, Y_tensors.size(1))] for i in range(0, oup_lens_tensor.shape[0])]))\n",
    "        agr_loss = torch.zeros(X_tensors.shape[0])\n",
    "        \n",
    "        if CUDA:\n",
    "            cont = cont.cuda()\n",
    "            coverage = coverage.cuda()\n",
    "            all_context_mask = all_context_mask.cuda()\n",
    "            agr_loss = agr_loss.cuda()\n",
    "        \n",
    "#         # Decoder\n",
    "#         all_dec_outputs = Variable(torch.zeros(output_train_max_len, X_tensors.shape[0], len(vocab_indexer)))   # sent_len, batch_size, ext_output_size\n",
    "#         for idx in range(output_train_max_len):\n",
    "#             p_gen, dec_output, dec_hidden, dec_attn = model_dec.forward(dec_input, dec_hidden, enc_outputs, enc_context_mask)\n",
    "#             all_dec_outputs[idx] = pointer_generate_train(p_gen, dec_output, dec_attn, X_tensors)\n",
    "#             max_prob_idx = torch.argmax(all_dec_outputs[idx], dim=1)\n",
    "#             print('max_prob_idx:', max_prob_idx)\n",
    "#             print('Y_tensors:', Y_tensors[:, idx])\n",
    "# #             print(agr_loss)\n",
    "# #             agr_loss = agr_loss + cal_step_loss(final_distrib, dec_attn, coverage, Y_tensors[:, idx].unsqueeze(1), all_context_mask[:, idx].float())\n",
    "#             dec_input = model_output_emb.forward(Y_tensors[:, idx])\n",
    "# #             coverage = next_coverage         \n",
    "#         loss = masked_cross_entropy(all_dec_outputs.transpose(0, 1).contiguous(), Y_tensors, oup_lens_tensor, all_context_mask)       # batch_size, sent_len, output_size\n",
    "        \n",
    "        \n",
    "        # Decoder2\n",
    "        for idx in range(output_train_max_len):\n",
    "            p_gen, dec_output, dec_hidden, dec_attn, cont, next_coverage = model_dec.forward(dec_input, dec_hidden, enc_outputs, enc_context_mask, cont, coverage)\n",
    "            final_distrib = pointer_generate_train(p_gen, dec_output, dec_attn, X_tensors, CUDA)\n",
    "#             max_prob_idx = torch.argmax(final_distrib, dim=1)\n",
    "#             print(final_distrib)\n",
    "#             print('max_prob_idx:', max_prob_idx)\n",
    "#             print('Y_tensors:', Y_tensors[:, idx])\n",
    "#             print(agr_loss)\n",
    "            agr_loss = agr_loss + cal_step_loss(final_distrib, dec_attn, coverage, Y_tensors[:, idx].unsqueeze(1), all_context_mask[:, idx].float())\n",
    "#             if torch.isnan(agr_loss).sum().item() > 0:\n",
    "#                 sys.exit(0)\n",
    "            dec_input = model_output_emb.forward(Y_tensors[:, idx])\n",
    "            coverage = next_coverage\n",
    "        \n",
    "        batch_avg_loss = agr_loss/oup_lens_tensor.float()\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(model_enc.parameters(), 0.25)\n",
    "        clip_grad_norm_(model_dec.parameters(), 0.25)\n",
    "\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "        print('loss', loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "elapsed_time = time.time() - start\n",
    "print('Time: %.2fs'%(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_enc_topic.pkl', 'wb') as fp:\n",
    "    pickle.dump(model_enc, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_dec_topic.pkl', 'wb') as fp:\n",
    "    pickle.dump(model_dec, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_inp_emb_topic.pkl', 'wb') as fp:\n",
    "    pickle.dump(model_input_emb, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_out_emb_topic.pkl', 'wb') as fp:\n",
    "    pickle.dump(model_output_emb, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_topic_losses.pkl', 'wb') as fp:\n",
    "    pickle.dump(losses, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Models/train_300_150_enc_no_topic.pkl', 'rb') as fp:\n",
    "    model_enc = pickle.load(fp)\n",
    "with open('./Models/train_300_150_dec_no_topic.pkl', 'rb') as fp:\n",
    "    model_dec = pickle.load(fp)\n",
    "with open('./Models/train_300_150_inp_emb_no_topic.pkl', 'rb') as fp:\n",
    "    model_input_emb = pickle.load(fp)\n",
    "with open('./Models/train_300_150_out_emb_no_topic.pkl', 'rb') as fp:\n",
    "    model_output_emb = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(lst):\n",
    "    lst.append(1)\n",
    "x = []\n",
    "func(x)\n",
    "x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(x):\n",
    "    print(i)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = Beam(2)\n",
    "# beam.add(['a', 'b'], 1)\n",
    "# beam.add(['c', 'd'], 2)\n",
    "# beam.add(['e', 'f'], 3)\n",
    "# beam.add(['g', 'h'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam.get_elts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elts = []\n",
    "elts=[[1],[2],[3]]\n",
    "new_elts = []\n",
    "for elt in elts:\n",
    "    for word_idx in [7,8,9]:\n",
    "        new_elt = elt + [word_idx]\n",
    "        new_elts.append(new_elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7], [1, 8], [1, 9], [2, 7], [2, 8], [2, 9], [3, 7], [3, 8], [3, 9]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_elts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entended Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_vocab_indexer = copy.deepcopy(vocab_indexer)\n",
    "for (x, y) in dev:\n",
    "    for x_tok, y_tok in zip(tokenize(x), tokenize(y)):\n",
    "        ext_vocab_indexer.get_index(x_tok)\n",
    "        ext_vocab_indexer.get_index(y_tok)\n",
    "        \n",
    "ext_dev_data_indexed = index_data(dev, ext_vocab_indexer)\n",
    "ext_dev_data_indexed.sort(key=lambda ex: len(ex.x_indexed), reverse=True)\n",
    "ext_input_dev_max_len = np.max(np.asarray([len(ex.x_indexed) for ex in ext_dev_data_indexed]))\n",
    "ext_all_dev_input_data = make_padded_input_tensor(ext_dev_data_indexed, ext_vocab_indexer, ext_input_dev_max_len).astype(np.int64)\n",
    "ext_X_tensors_batch_dev = batch_data(ext_all_dev_input_data, BATCH_SIZE, cuda=CUDA)   # batch_num, batch_size, sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Copy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointer_generate_dev(p_gen, dec_output, dec_attn, ext_X_tensors, vocab_indexer, ext_vocab_indexer, cuda):\n",
    "    final_distrib = torch.zeros([dec_output.shape[0], len(ext_vocab_indexer)], dtype=torch.float)    # add new words in the end\n",
    "    if cuda:\n",
    "        final_distrib = final_distrib.cuda()\n",
    "    final_distrib[:, 0:len(vocab_indexer)] = p_gen * dec_output\n",
    "    dec_attn_padding = torch.zeros(ext_X_tensors.shape, dtype=torch.float)\n",
    "    if cuda:\n",
    "        dec_attn_padding = dec_attn_padding.cuda()\n",
    "    dec_attn_padding[:, 0:dec_attn.shape[1]] = dec_attn\n",
    "    final_distrib = final_distrib.scatter_add(1, ext_X_tensors, (1-p_gen)*dec_attn_padding)\n",
    "    final_distrib[:, vocab_indexer.index_of(UNK_SYMBOL)] = 0\n",
    "    return final_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPerf(scores_lst):\n",
    "    result_dic = {'rouge-1': {'f':0, 'p':0, 'r':0}, 'rouge-2': {'f':0, 'p':0, 'r':0}, 'rouge-l': {'f':0, 'p':0, 'r':0}}\n",
    "    for scores in scores_lst:\n",
    "        for key1 in scores:\n",
    "            for key2 in scores[key1]:\n",
    "                result_dic[key1][key2] += scores[key1][key2]\n",
    "    sent_nums = len(scores_lst)\n",
    "    for key1 in result_dic:\n",
    "        for key2 in result_dic[key1]:\n",
    "            result_dic[key1][key2] /= sent_nums\n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create indexed input/output for development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexed input/output for dev\n",
    "dev_data_indexed.sort(key=lambda ex: len(ex.x_indexed), reverse=True)\n",
    "input_dev_max_len = np.max(np.asarray([len(ex.x_indexed) for ex in dev_data_indexed]))\n",
    "all_dev_input_data = make_padded_input_tensor(dev_data_indexed, vocab_indexer, input_dev_max_len).astype(np.int64)\n",
    "output_dev_max_len = np.max(np.asarray([len(ex.y_indexed) for ex in dev_data_indexed]))\n",
    "X_tensors_batch_dev = batch_data(all_dev_input_data, BATCH_SIZE, cuda=CUDA)   # batch_num, batch_size, sent_len\n",
    "pos_tensor_batch = []\n",
    "for X_tensor in X_tensors_batch_dev:\n",
    "    pos_tensor = []\n",
    "    for i in range(X_tensor.shape[0]):\n",
    "        for j in range(X_tensor.shape[1]):\n",
    "            token = X_tensor[i][j]\n",
    "            if token in topic_vocab:\n",
    "                pos_tensor.append([i, j])\n",
    "    pos_tensor_batch.append(pos_tensor) \n",
    "if CUDA:\n",
    "    inp_lens_batch_dev = [torch.tensor([torch.sum(X_tensor != 0) for X_tensor in X_tensors]).cuda() for X_tensors in X_tensors_batch_dev]  # batch_num, batch_size\n",
    "else:\n",
    "    inp_lens_batch_dev = [torch.tensor([torch.sum(X_tensor != 0) for X_tensor in X_tensors]) for X_tensors in X_tensors_batch_dev]  # batch_num, batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_index(step_probs, sent_ngram):\n",
    "    max_prob_idx = torch.argmax(step_probs, dim=1)\n",
    "#     if sent_ngram is None:\n",
    "#         return max_prob_idx\n",
    "#     for sent_idx in range(len(sent_ngram)):\n",
    "#         if sent_ngram[sent_idx] == max_prob_idx[sent_idx]:\n",
    "# #             print('pre:', torch.sort(step_probs[sent_idx], descending=True)[1][1].item())\n",
    "#             max_prob_idx[sent_idx] = torch.sort(step_probs[sent_idx], descending=True)[1][1]\n",
    "# #             print('aft:', max_prob_idx[sent_idx])\n",
    "    return max_prob_idx      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_data = []\n",
    "model_enc.eval()\n",
    "model_dec.eval()\n",
    "if TOPIC:\n",
    "    tokens_embedding = model_input_emb.forward(tokens)\n",
    "    topic_embedding = topic_word.mm(tokens_embedding)\n",
    "for X_tensors, pos_tensor, inp_lens_tensor, ext_X_tensors in zip(X_tensors_batch_dev, pos_tensor_batch, inp_lens_batch_dev, ext_X_tensors_batch_dev):\n",
    "#     print(\"%d/%d\"%(cnt+1, total), end='\\r')\n",
    "#     cnt += 1\n",
    "    if TOPIC:\n",
    "        enc_outputs, enc_context_mask, enc_hidden = encode_input_for_decoder(X_tensors, inp_lens_tensor, model_input_emb, model_enc, topic_embedding, pos_tensor)\n",
    "    else:\n",
    "        enc_outputs, enc_context_mask, enc_hidden = encode_input_for_decoder(X_tensors, inp_lens_tensor, model_input_emb, model_enc)\n",
    "    dec_hidden = enc_hidden\n",
    "    init_dec_inp = Variable(torch.LongTensor([vocab_indexer.index_of(SOS_SYMBOL)] * X_tensors.shape[0]))  \n",
    "    if CUDA:\n",
    "        init_dec_inp = init_dec_inp.cuda()\n",
    "    dec_input = model_output_emb.forward(init_dec_inp)\n",
    "    cont = torch.zeros((X_tensors.shape[0], 2 * hidden_size))    # batch_size, 2*hidden_size\n",
    "    coverage = torch.zeros((X_tensors.shape[0], enc_outputs.shape[0]))             # batch_size, sent_lens\n",
    "    max_prob_batch = torch.zeros(X_tensors.shape[0], output_dev_max_len)       # batch_size, sent_lens\n",
    "#     all_dec_outputs = Variable(torch.zeros(output_dev_max_len, X_tensors.shape[0], len(ext_vocab_indexer)))   # sent_len, batch_size, ext_output_size\n",
    "    \n",
    "    if CUDA:\n",
    "        cont = cont.cuda()\n",
    "        coverage = coverage.cuda()\n",
    "#         all_dec_outputs = all_dec_outputs.cuda()\n",
    "    \n",
    "    sent_ngram = None\n",
    "    for idx in range(output_dev_max_len):\n",
    "        p_gen, dec_output, dec_hidden, dec_attn, cont, next_coverage = model_dec.forward(dec_input, dec_hidden, enc_outputs, enc_context_mask, cont, coverage)\n",
    "        final_distrib = pointer_generate_dev(p_gen, dec_output, dec_attn, ext_X_tensors, vocab_indexer, ext_vocab_indexer, CUDA)\n",
    "        max_prob_idx = find_max_index(final_distrib, sent_ngram)\n",
    "        sent_ngram = max_prob_idx\n",
    "        max_prob_batch[:, idx] = max_prob_idx\n",
    "        max_prob_idx[max_prob_idx >= len(vocab_indexer)] = vocab_indexer.index_of(UNK_SYMBOL)   # new words should be UNK when serving as next input\n",
    "        dec_input = model_output_emb.forward(max_prob_idx)\n",
    "    \n",
    "    for best_sent in max_prob_batch:\n",
    "        best_ex = []\n",
    "        for word_idx in best_sent:            # don't need to include EOS tok\n",
    "            if word_idx.item() == ext_vocab_indexer.index_of(EOS_SYMBOL):\n",
    "                break\n",
    "            best_ex.append(ext_vocab_indexer.get_object(word_idx.item()))     # pred tok\n",
    "        best_data.append(best_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROUGE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "scores_lst = []\n",
    "doc_str_lst = []\n",
    "test_str_lst = []\n",
    "best_str_lst = []\n",
    "for test_ex, best_ex in zip(dev_data_indexed, best_data):\n",
    "    doc_str = ' '.join(test_ex.x_tok)\n",
    "    test_str = ' '.join(test_ex.y_tok)\n",
    "    best_str = ' '.join(best_ex)\n",
    "    print(doc_str)\n",
    "    print(test_str)\n",
    "    print(best_str)\n",
    "    test_str_lst.append(test_str)\n",
    "    best_str_lst.append(best_str)\n",
    "    doc_str_lst.append(doc_str)\n",
    "    scores = rouge.get_scores(best_str, test_str)\n",
    "    print(scores)\n",
    "    scores_lst.append(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = getModelPerf(scores_lst)\n",
    "print(result_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doc\", \"w\", encoding='utf8') as fp:\n",
    "    for line in doc_str_lst:\n",
    "        fp.write(line)\n",
    "        fp.write(\"\\n\")\n",
    "with open(\"test\", \"w\", encoding='utf8') as fp:\n",
    "    for line in best_str_lst:\n",
    "        fp.write(line)\n",
    "        fp.write(\"\\n\")\n",
    "with open(\"reference\", \"w\", encoding='utf8') as fp:\n",
    "    for line in test_str_lst:\n",
    "        fp.write(line)\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METEOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact match mode: rewarding only exact matches between words\n",
    "print(check_output(\"java -Xmx2G -jar meteor-1.5.jar test reference -l en -norm -m exact\", shell=True).decode(\"utf-8\"))\n",
    "# full mode: additionally rewards matching stems, synonyms and paraphrases\n",
    "print(check_output('java -Xmx2G -jar meteor-1.5.jar test reference -l en -norm -m \"exact stem synonym paraphrase\"', shell=True).decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
